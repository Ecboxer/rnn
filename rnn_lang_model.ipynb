{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T14:03:59.815773Z",
     "start_time": "2018-12-26T14:03:59.647849Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T14:05:01.190555Z",
     "start_time": "2018-12-26T14:05:01.186081Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T14:04:25.198339Z",
     "start_time": "2018-12-26T14:04:10.510313Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 1500\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "sentence_start_token = 'SENTENCE_START'\n",
    "sentence_end_token = 'SENTENCE_END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "f = open('wizOfOz.txt', 'r')\n",
    "#Read corpus into text as list of lines\n",
    "ast_count = 0\n",
    "for line in f.readlines():\n",
    "    if ast_count == 0: #Gutenberg text\n",
    "        if line[0] == '*':\n",
    "            ast_count += 1\n",
    "    elif ast_count == 1: #Book text\n",
    "        if line[0] == '*': #Gutenberg text\n",
    "            ast_count += 1\n",
    "        else:\n",
    "            text.append(line)\n",
    "f.close()\n",
    "\n",
    "#Form corpus by joining list of lines\n",
    "corpus = ''.join(text)\n",
    "\n",
    "#Remove line breaks and returns\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "corpus = corpus.replace('\\r', '')\n",
    "corpus = corpus.replace('\\\\', '')\n",
    "\n",
    "#Start from the introduction, the second occurrence of the word Introduction\n",
    "iter = re.finditer(r'Introduction', corpus)\n",
    "intro_indices = [m.start(0) for m in iter]\n",
    "corp = corpus[intro_indices[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split corpus in to sentences\n",
    "sentences = nltk.sent_tokenize(corp.decode('utf-8').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append SENTENCE_START and SENTENCE_END\n",
    "sentences = ['%s %s %s' % (sentence_start_token, x, sentence_end_token) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2223 sentences\n"
     ]
    }
   ],
   "source": [
    "print 'Parsed %d sentences' % (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2959 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "#Count word frequency\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print 'Found %d unique word tokens' % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 1500\n",
      "The least frequent word in our vocabulary is \"hurry\" appearing 2 times\n"
     ]
    }
   ],
   "source": [
    "#Get the most common words and build idx2word and word2idx vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "idx2word = [x[0] for x in vocab]\n",
    "idx2word.append(unknown_token)\n",
    "word2idx = dict([(w,i) for i,w in enumerate(idx2word)])\n",
    "print 'Using vocabulary size %d' % vocabulary_size\n",
    "print 'The least frequent word in our vocabulary is \"%s\" appearing %d times' % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all words not in our vocabulary with the unknown token\n",
    "for i,sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word2idx else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: \"SENTENCE_START and if he is the great head, he will be at my mercy; for i will roll this head all about the room until he promises to give us what we desire. SENTENCE_END\"\n",
      "Example sentence after pre-processing \"[u'SENTENCE_START', u'and', u'if', u'he', u'is', u'the', u'great', u'head', u',', u'he', u'will', u'be', u'at', u'my', 'UNKNOWN_TOKEN', u';', u'for', u'i', u'will', u'roll', u'this', u'head', u'all', u'about', u'the', u'room', u'until', u'he', u'promises', u'to', u'give', u'us', u'what', u'we', u'desire', u'.', u'SENTENCE_END']\"\n"
     ]
    }
   ],
   "source": [
    "temp = np.random.randint(0, len(sentences))\n",
    "print 'Example sentence: \"%s\"\\nExample sentence after pre-processing \"%s\"' % (sentences[temp], tokenized_sentences[temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training data\n",
    "X_train = np.asarray([[word2idx[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word2idx[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START `` this funny tin man , '' she answered , `` killed the wildcat and saved my life .\n",
      "[3, 7, 52, 992, 55, 100, 1, 8, 19, 98, 1, 7, 327, 0, 774, 5, 648, 40, 337, 4]\n",
      "y:\n",
      "`` this funny tin man , '' she answered , `` killed the wildcat and saved my life . SENTENCE_END\n",
      "[7, 52, 992, 55, 100, 1, 8, 19, 98, 1, 7, 327, 0, 774, 5, 648, 40, 337, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "#Training data example\n",
    "temp = np.random.randint(0, len(sentences))\n",
    "x_example, y_example = X_train[temp], y_train[temp]\n",
    "print 'x:\\n%s\\n%s' % (' '.join([idx2word[x] for x in x_example]), x_example)\n",
    "print 'y:\\n%s\\n%s' % (' '.join([idx2word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s_t = $tanh$(Ux_t + Ws_{t-1})$  \n",
    "$o_t = $softmax$(Vs_t)$  \n",
    "Let our hidden layer have size $H = 100$  \n",
    "$x_t \\in \\mathbb{R}^{1500}$  \n",
    "$o_t \\in \\mathbb{R}^{1500}$  \n",
    "$s_t \\in \\mathbb{R}^{100}$  \n",
    "$U \\in \\mathbb{R}^{100 \\times 1500}$  \n",
    "$V \\in \\mathbb{R}^{1500 \\times 100}$  \n",
    "$W \\in \\mathbb{R}^{100 \\times 100}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        #Random initialization of network parameters in the range [-n**(-1/2), n**(-1/2)]\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    #Total number of time steps\n",
    "    T = len(x)\n",
    "    #Save hidden states in s for later\n",
    "    #Add one additional element for the initial hidden state, set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    #Outputs at each time step are saved for later\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    #At each time step\n",
    "    for t in np.arange(T):\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = np_softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    #Perform forward propagation and return index of highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1500)\n",
      "[[ 0.00067586  0.00067626  0.00067335 ...,  0.00066474  0.0006725\n",
      "   0.00066804]\n",
      " [ 0.0006685   0.00066554  0.00066936 ...,  0.00066188  0.0006709\n",
      "   0.00067597]\n",
      " [ 0.00066997  0.00066798  0.00065605 ...,  0.00067054  0.00065682\n",
      "   0.00066726]\n",
      " ..., \n",
      " [ 0.00067106  0.00066196  0.00066688 ...,  0.00065991  0.00066763\n",
      "   0.00066349]\n",
      " [ 0.00066872  0.00065826  0.00067526 ...,  0.000666    0.00067557\n",
      "   0.00066162]\n",
      " [ 0.00068957  0.00066697  0.00066813 ...,  0.00066418  0.00068742\n",
      "   0.00066253]]\n"
     ]
    }
   ],
   "source": [
    "#Example output\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "[ 284  786   87  399   63  924   95  999  577 1035   23  110  721 1109  212\n",
      " 1228  741 1260 1273  733  875  853  104  345  683 1251  189 1273  938  107\n",
      "  870 1219  332  569 1379 1415  935  741 1480 1485  741  581  175  938  605\n",
      "  623 1445  746  569  144]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print predictions.shape\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'started paved did ground could visit now fewer edge rushed as heart quick pure along bend grandfather polished worth careful shut sweet after look known doorway friends worth save your sit flowing although whiskers rising underneath gulf grandfather building condition grandfather rusted some save against bigger ruined kindly whiskers walked'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[i] for i in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    #For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        #Determine the correct word predictions\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        #Add to the loss based on the cross-entropy each term\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    #Divide total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for random predictions: 7.313220\n",
      "Actual loss: 7.313423\n"
     ]
    }
   ],
   "source": [
    "#Loss for 1000 examples\n",
    "print 'Expected loss for random predictions: %f' % np.log(vocabulary_size)\n",
    "print 'Actual loss: %f' % model.calculate_loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the RNN through SGD and BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    #Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    #Gradient accumulators\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1\n",
    "    #For each output, backwards ...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        #Initial delta\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        #Backpropagation through time (for at most self.bptt truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            #print 'Backpropagation step t=%d bptt step=%d' % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            #Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1]**2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    #Calculate gradients using backpropagation and check whether that are correct\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    #Parameters to check\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        #Get actual parameter value from the model\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print 'Performing gradient check for parameter %s with size %d' % (pname, np.prod(parameter.shape))\n",
    "        #Iterate over each element of the parameter matrix\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            #Save original value to be reset to later\n",
    "            original_value = parameter[ix]\n",
    "            #Estimate the gradients as (f(x+h) - f(x-h)) / (2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x], [y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x], [y])\n",
    "            estimated_gradient = (gradplus - gradminus) / (2*h)\n",
    "            #Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            #Calculate the gradient for this parameter with backprop\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            #Calculate the relative error (|x - y| / (|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            #If the error is too large, then fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print 'Gradient check ERROR: parameter=%s ix=%s' % (pname, ix)\n",
    "                print '+h loss: %f' % gradplus\n",
    "                print '-h loss: %f' % gradminus\n",
    "                print 'Estimated gradient: %f' % estimated_gradient\n",
    "                print 'Backpropagation gradient: %f' % backprop_gradient\n",
    "                print 'Relative Error: %f' % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        print 'Gradient check for parameter %s passed' % pname\n",
    "        \n",
    "RNNNumpy.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000\n",
      "Gradient check for parameter U passed\n",
      "Performing gradient check for parameter V with size 1000\n",
      "Gradient check for parameter V passed\n",
      "Performing gradient check for parameter W with size 100\n",
      "Gradient check for parameter W passed\n"
     ]
    }
   ],
   "source": [
    "#Check with smaller vocabulary size\n",
    "grad_check_vocab_size = 100\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two step implementation:  \n",
    "1. `sdg_step` calculates the gradients and performs updates for one batch  \n",
    "2. Outer loop that iterates through the training set and adjusts the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_sgd_step(self, x, y, learning_rate):\n",
    "    #Calculate gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    #Change parameters according to the gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "    \n",
    "RNNNumpy.sgd_step = numpy_sgd_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sdg(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    \"\"\"Outer SGD Loop\n",
    "    params:\n",
    "    X_train: training data set\n",
    "    y_train: training data targets\n",
    "    learning_rate: initial learning rate for SGD\n",
    "    nepoch: number of times to iterate throught the complete data set\n",
    "    evaluate_loss_after: evaluate the loss after this many epochs\"\"\"\n",
    "    #Keep track of losses\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        #Optionally evaluate loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print '%s: Loss after num_examples_seen=%d epoch=%d: %f' % (time, num_examples_seen, epoch, loss)\n",
    "            #Adjust the learning rate, if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print 'Setting learning rate to %f' % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        #For each training example ...\n",
    "        for i in range(len(y_train)):\n",
    "            #One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 37.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27 10:50:04: Loss after num_examples_seen=0 epoch=0: 7.313290\n",
      "2018-12-27 10:50:06: Loss after num_examples_seen=100 epoch=1: 7.296705\n",
      "2018-12-27 10:50:09: Loss after num_examples_seen=200 epoch=2: 7.251789\n",
      "2018-12-27 10:50:11: Loss after num_examples_seen=300 epoch=3: 5.484497\n",
      "2018-12-27 10:50:13: Loss after num_examples_seen=400 epoch=4: 5.328043\n",
      "2018-12-27 10:50:15: Loss after num_examples_seen=500 epoch=5: 5.243652\n",
      "2018-12-27 10:50:19: Loss after num_examples_seen=600 epoch=6: 5.170964\n",
      "2018-12-27 10:50:22: Loss after num_examples_seen=700 epoch=7: 5.106834\n",
      "2018-12-27 10:50:26: Loss after num_examples_seen=800 epoch=8: 5.065418\n",
      "2018-12-27 10:50:29: Loss after num_examples_seen=900 epoch=9: 5.038469\n"
     ]
    }
   ],
   "source": [
    "#Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sdg(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network with Theano and the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano as theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        # Theano: Created shared variables\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))      \n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        U, V, W = self.U, self.V, self.W\n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        def forward_prop_step(x_t, s_t_prev, U, V, W):\n",
    "            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n",
    "            o_t = T.nnet.softmax(V.dot(s_t))\n",
    "            return [o_t[0], s_t]\n",
    "        [o,s], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x,\n",
    "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
    "            non_sequences=[U, V, W],\n",
    "            truncate_gradient=self.bptt_truncate,\n",
    "            strict=True)\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Gradients\n",
    "        dU = T.grad(o_error, U)\n",
    "        dV = T.grad(o_error, V)\n",
    "        dW = T.grad(o_error, W)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.forward_propagation = theano.function([x], o)\n",
    "        self.predict = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], o_error)\n",
    "        self.bptt = theano.function([x, y], [dU, dV, dW])\n",
    "        \n",
    "        # SGD\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        self.sgd_step = theano.function([x,y,learning_rate], [], \n",
    "                      updates=[(self.U, self.U - learning_rate * dU),\n",
    "                              (self.V, self.V - learning_rate * dV),\n",
    "                              (self.W, self.W - learning_rate * dW)])\n",
    "    \n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_total_loss(X,Y)/float(num_words)   \n",
    "\n",
    "\n",
    "def gradient_check_theano(model, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Overwrite the bptt attribute. We need to backpropagate all the way to get the correct gradient\n",
    "    model.bptt_truncate = 1000\n",
    "    # Calculate the gradients using backprop\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to chec.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter_T = operator.attrgetter(pname)(model)\n",
    "        parameter = parameter_T.get_value()\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            parameter_T.set_value(parameter)\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            parameter_T.set_value(parameter)\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            parameter[ix] = original_value\n",
    "            parameter_T.set_value(parameter)\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 50.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 50.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 5\n",
    "model = RNNTheano(grad_check_vocab_size, 10)\n",
    "gradient_check_theano(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 15.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "model = RNNTheano(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27 10:51:16: Loss after num_examples_seen=0 epoch=0: 7.312975\n",
      "2018-12-27 10:55:24: Loss after num_examples_seen=22230 epoch=10: 6.605725\n",
      "2018-12-27 10:59:37: Loss after num_examples_seen=44460 epoch=20: 5.902666\n",
      "2018-12-27 11:03:38: Loss after num_examples_seen=66690 epoch=30: 5.824765\n",
      "2018-12-27 11:07:31: Loss after num_examples_seen=88920 epoch=40: 6.170466\n",
      "Setting learning rate to 0.002500\n",
      "2018-12-27 11:11:14: Loss after num_examples_seen=111150 epoch=50: 5.699436\n",
      "2018-12-27 11:14:59: Loss after num_examples_seen=133380 epoch=60: 5.737881\n",
      "Setting learning rate to 0.001250\n",
      "2018-12-27 11:18:45: Loss after num_examples_seen=155610 epoch=70: 5.719997\n",
      "2018-12-27 11:22:26: Loss after num_examples_seen=177840 epoch=80: 5.780091\n",
      "Setting learning rate to 0.000625\n",
      "2018-12-27 11:26:11: Loss after num_examples_seen=200070 epoch=90: 5.824584\n",
      "Setting learning rate to 0.000313\n"
     ]
    }
   ],
   "source": [
    "model = RNNTheano(vocabulary_size, hidden_dim=100)\n",
    "losses = train_with_sdg(model, X_train, y_train, nepoch=100, evaluate_loss_after=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_dim 50: Takes about 15 minutes, but reached loss of 5.7 after 50 epochs and 8 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model parameters to data/trained_model_theano.npz.\n"
     ]
    }
   ],
   "source": [
    "save_model_parameters_theano('data/trained_model_theano.npz', model)\n",
    "#load_model_parameters_theano('data/trained_model_theano.npz', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    #Begin with the start token\n",
    "    new_sentence = [word2idx[sentence_start_token]]\n",
    "    #Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word2idx[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word2idx[unknown_token]\n",
    "        #We don't want to sample unknown words\n",
    "        while sampled_word == word2idx[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [idx2word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a princess china little also ? ''\n",
      "she asked , his thick ! ''\n",
      "she curiosity child her head ? ''\n",
      "cried the girl like of do roof .\n",
      "she asked , i sorrow out ! ''\n",
      "asked dorothy eagerly to kansas . ''\n",
      "center back to you thing back ! ''\n",
      "asked dorothy lost saw dorothy witch his better .\n",
      "hill , in you directions ! ''\n",
      "asked the swim lion give her draw .\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "sentence_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    #We want long sentences\n",
    "    while len(sent) < sentence_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:  \n",
    "* Learned to end sentences with punctuation  \n",
    "* Learned some adjectives that appear together often, eg tin woodman, little wizard  \n",
    "\n",
    "Cons:  \n",
    "* Quotation marks do not always end and do not make contextual sense  \n",
    "* Nonsense sentences, ie there is no context to the word choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` asked not found unhappy green as imagined . ''\n",
      "said the farmer in rusted feet perhaps form from the emerald city . ''\n",
      "she asked it draw him head and broad it . ''\n",
      "asked the lion prairie do and a heart . ''\n",
      "tin mouse so glasses in his head green me to it .\n",
      "asked the scarecrow lion woman do the wall . ''\n",
      "cried dorothy will her our at do good way . ''\n",
      "asked dorothy on in had cupboard before ever sadly .\n",
      "asked the child woodman chop idle it happy . ''\n",
      "asked the milkmaid thoughtfully , with the world of the basket .\n",
      "its struck before own were as at the others side . ''\n",
      "asked the scarecrow in the cat their clothes . ''\n",
      "child the lion to do you would more . ''\n",
      "she said dorothy knows there horses to her sorceress .\n",
      "she same boq from bad still hair so even .\n",
      "good-bye i angrily the clouds trick have evidently her way .\n",
      "cried the little in the guarded very with him glinda .\n",
      "she asked me of over my head it skins . ''\n",
      "asked the farmer anxiously fill you reached and journey upon it leave together .\n",
      "answered the princess stopped 's instantly whenever with knocked new disagreeable shoulders .\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "sentence_min_length = 10\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    #We want long sentences\n",
    "    while len(sent) < sentence_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
