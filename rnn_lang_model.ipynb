{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T14:03:59.815773Z",
     "start_time": "2018-12-26T14:03:59.647849Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T14:05:01.190555Z",
     "start_time": "2018-12-26T14:05:01.186081Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T14:04:25.198339Z",
     "start_time": "2018-12-26T14:04:10.510313Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to /home/eric/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/eric/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 1500\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "sentence_start_token = 'SENTENCE_START'\n",
    "sentence_end_token = 'SENTENCE_END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "f = open('wizOfOz.txt', 'r')\n",
    "#Read corpus into text as list of lines\n",
    "ast_count = 0\n",
    "for line in f.readlines():\n",
    "    if ast_count == 0: #Gutenberg text\n",
    "        if line[0] == '*':\n",
    "            ast_count += 1\n",
    "    elif ast_count == 1: #Book text\n",
    "        if line[0] == '*': #Gutenberg text\n",
    "            ast_count += 1\n",
    "        else:\n",
    "            text.append(line)\n",
    "f.close()\n",
    "\n",
    "#Form corpus by joining list of lines\n",
    "corpus = ''.join(text)\n",
    "\n",
    "#Remove line breaks and returns\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "corpus = corpus.replace('\\r', '')\n",
    "corpus = corpus.replace('\\\\', '')\n",
    "\n",
    "#Start from the introduction, the second occurrence of the word Introduction\n",
    "iter = re.finditer(r'Introduction', corpus)\n",
    "intro_indices = [m.start(0) for m in iter]\n",
    "corp = corpus[intro_indices[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split corpus in to sentences\n",
    "sentences = nltk.sent_tokenize(corp.decode('utf-8').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append SENTENCE_START and SENTENCE_END\n",
    "sentences = ['%s %s %s' % (sentence_start_token, x, sentence_end_token) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2223 sentences\n"
     ]
    }
   ],
   "source": [
    "print 'Parsed %d sentences' % (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2959 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "#Count word frequency\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print 'Found %d unique word tokens' % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 1500\n",
      "The least frequent word in our vocabulary is \"hurry\" appearing 2 times\n"
     ]
    }
   ],
   "source": [
    "#Get the most common words and build idx2word and word2idx vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "idx2word = [x[0] for x in vocab]\n",
    "idx2word.append(unknown_token)\n",
    "word2idx = dict([(w,i) for i,w in enumerate(idx2word)])\n",
    "print 'Using vocabulary size %d' % vocabulary_size\n",
    "print 'The least frequent word in our vocabulary is \"%s\" appearing %d times' % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all words not in our vocabulary with the unknown token\n",
    "for i,sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word2idx else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: \"SENTENCE_START she was dressed in green silk gauze and wore upon her flowing green locks a crown of jewels. SENTENCE_END\"\n",
      "Example sentence after pre-processing \"[u'SENTENCE_START', u'she', u'was', u'dressed', u'in', u'green', u'silk', 'UNKNOWN_TOKEN', u'and', u'wore', u'upon', u'her', u'flowing', u'green', 'UNKNOWN_TOKEN', u'a', 'UNKNOWN_TOKEN', u'of', u'jewels', u'.', u'SENTENCE_END']\"\n"
     ]
    }
   ],
   "source": [
    "temp = np.random.randint(0, len(sentences))\n",
    "print 'Example sentence: \"%s\"\\nExample sentence after pre-processing \"%s\"' % (sentences[temp], tokenized_sentences[temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training data\n",
    "X_train = np.asarray([[word2idx[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word2idx[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START but he at once unlocked their spectacles , which he put back into the green box , and gave them many good wishes to carry with them .\n",
      "[3, 26, 15, 31, 131, 1152, 73, 492, 1, 112, 15, 183, 74, 97, 0, 72, 806, 1, 5, 195, 33, 138, 101, 693, 6, 221, 28, 33, 4]\n",
      "y:\n",
      "but he at once unlocked their spectacles , which he put back into the green box , and gave them many good wishes to carry with them . SENTENCE_END\n",
      "[26, 15, 31, 131, 1152, 73, 492, 1, 112, 15, 183, 74, 97, 0, 72, 806, 1, 5, 195, 33, 138, 101, 693, 6, 221, 28, 33, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "#Training data example\n",
    "temp = np.random.randint(0, len(sentences))\n",
    "x_example, y_example = X_train[temp], y_train[temp]\n",
    "print 'x:\\n%s\\n%s' % (' '.join([idx2word[x] for x in x_example]), x_example)\n",
    "print 'y:\\n%s\\n%s' % (' '.join([idx2word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s_t = $tanh$(Ux_t + Ws_{t-1})$  \n",
    "$o_t = $softmax$(Vs_t)$  \n",
    "Let our hidden layer have size $H = 100$  \n",
    "$x_t \\in \\mathbb{R}^{1500}$  \n",
    "$o_t \\in \\mathbb{R}^{1500}$  \n",
    "$s_t \\in \\mathbb{R}^{100}$  \n",
    "$U \\in \\mathbb{R}^{100 \\times 1500}$  \n",
    "$V \\in \\mathbb{R}^{1500 \\times 100}$  \n",
    "$W \\in \\mathbb{R}^{100 \\times 100}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        #Random initialization of network parameters in the range [-n**(-1/2), n**(-1/2)]\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    #Total number of time steps\n",
    "    T = len(x)\n",
    "    #Save hidden states in s for later\n",
    "    #Add one additional element for the initial hidden state, set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    #Outputs at each time step are saved for later\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    #At each time step\n",
    "    for t in np.arange(T):\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = np_softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    #Perform forward propagation and return index of highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1500)\n",
      "[[ 0.00067235  0.00066777  0.00066659 ...,  0.00066377  0.00066365\n",
      "   0.00067008]\n",
      " [ 0.00067458  0.00067966  0.00066873 ...,  0.00068018  0.00065385\n",
      "   0.00066934]\n",
      " [ 0.00066685  0.00065744  0.00067244 ...,  0.00067043  0.0006741\n",
      "   0.00064967]\n",
      " ..., \n",
      " [ 0.00066946  0.00067601  0.00066425 ...,  0.00066204  0.00067731\n",
      "   0.0006514 ]\n",
      " [ 0.00067488  0.00066734  0.00066883 ...,  0.00067339  0.00065754\n",
      "   0.00066401]\n",
      " [ 0.00066567  0.00066278  0.00066482 ...,  0.00066243  0.0006694\n",
      "   0.0006709 ]]\n"
     ]
    }
   ],
   "source": [
    "#Example output\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "[ 979  397  529  296 1398  156  203  769  764 1160  740 1428 1123 1042  219\n",
      "  822  506  660  691 1071 1270  475 1042 1155  957  974  381  608 1157  813\n",
      "  726   46  827  225  830 1096  294  972 1412  769 1133  465  241  228  199\n",
      " 1398 1374  865  132 1257]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print predictions.shape\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"mishap dear strong each usually went journey almost gently bounded children oats hungry path three stretched covered rolled 've suggested gingham without path calmly dreadful pleasant kill bottom strip pulled lonely do seems cried chop managed bed beyond troubles almost dangers best small sure next usually dotted presently thought loss\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[i] for i in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    #For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        #Determine the correct word predictions\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        #Add to the loss based on the cross-entropy each term\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    #Divide total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for random predictions: 7.313220\n",
      "Actual loss: 7.313597\n"
     ]
    }
   ],
   "source": [
    "#Loss for 1000 examples\n",
    "print 'Expected loss for random predictions: %f' % np.log(vocabulary_size)\n",
    "print 'Actual loss: %f' % model.calculate_loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the RNN through SGD and BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    #Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    #Gradient accumulators\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1\n",
    "    #For each output, backwards ...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        #Initial delta\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        #Backpropagation through time (for at most self.bptt truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            #print 'Backpropagation step t=%d bptt step=%d' % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            #Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1]**2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    #Calculate gradients using backpropagation and check whether that are correct\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    #Parameters to check\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        #Get actual parameter value from the model\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print 'Performing gradient check for parameter %s with size %d' % (pname, np.prod(parameter.shape))\n",
    "        #Iterate over each element of the parameter matrix\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            #Save original value to be reset to later\n",
    "            original_value = parameter[ix]\n",
    "            #Estimate the gradients as (f(x+h) - f(x-h)) / (2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x], [y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x], [y])\n",
    "            estimated_gradient = (gradplus - gradminus) / (2*h)\n",
    "            #Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            #Calculate the gradient for this parameter with backprop\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            #Calculate the relative error (|x - y| / (|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            #If the error is too large, then fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print 'Gradient check ERROR: parameter=%s ix=%s' % (pname, ix)\n",
    "                print '+h loss: %f' % gradplus\n",
    "                print '-h loss: %f' % gradminus\n",
    "                print 'Estimated gradient: %f' % estimated_gradient\n",
    "                print 'Backpropagation gradient: %f' % backprop_gradient\n",
    "                print 'Relative Error: %f' % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        print 'Gradient check for parameter %s passed' % pname\n",
    "        \n",
    "RNNNumpy.gradient_check = gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000\n",
      "Gradient check for parameter U passed\n",
      "Performing gradient check for parameter V with size 1000\n",
      "Gradient check for parameter V passed\n",
      "Performing gradient check for parameter W with size 100\n",
      "Gradient check for parameter W passed\n"
     ]
    }
   ],
   "source": [
    "#Check with smaller vocabulary size\n",
    "grad_check_vocab_size = 100\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two step implementation:  \n",
    "1. `sdg_step` calculates the gradients and performs updates for one batch  \n",
    "2. Outer loop that iterates through the training set and adjusts the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_sgd_step(self, x, y, learning_rate):\n",
    "    #Calculate gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    #Change parameters according to the gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "    \n",
    "RNNNumpy.sgd_step = numpy_sgd_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sdg(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    \"\"\"Outer SGD Loop\n",
    "    params:\n",
    "    X_train: training data set\n",
    "    y_train: training data targets\n",
    "    learning_rate: initial learning rate for SGD\n",
    "    nepoch: number of times to iterate throught the complete data set\n",
    "    evaluate_loss_after: evaluate the loss after this many epochs\"\"\"\n",
    "    #Keep track of losses\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        #Optionally evaluate loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print '%s: Loss after num_examples_seen=%d epoch=%d: %f' % (time, num_examples_seen, epoch, loss)\n",
    "            #Adjust the learning rate, if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print 'Setting learning rate to %f' % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        #For each training example ...\n",
    "        for i in range(len(y_train)):\n",
    "            #One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 38.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26 16:05:20: Loss after num_examples_seen=0 epoch=0: 7.313384\n",
      "2018-12-26 16:05:22: Loss after num_examples_seen=100 epoch=1: 7.296847\n",
      "2018-12-26 16:05:25: Loss after num_examples_seen=200 epoch=2: 6.163766\n",
      "2018-12-26 16:05:27: Loss after num_examples_seen=300 epoch=3: 5.445909\n",
      "2018-12-26 16:05:29: Loss after num_examples_seen=400 epoch=4: 5.312528\n",
      "2018-12-26 16:05:31: Loss after num_examples_seen=500 epoch=5: 5.231207\n",
      "2018-12-26 16:05:33: Loss after num_examples_seen=600 epoch=6: 5.155260\n",
      "2018-12-26 16:05:35: Loss after num_examples_seen=700 epoch=7: 5.094630\n",
      "2018-12-26 16:05:37: Loss after num_examples_seen=800 epoch=8: 5.057536\n",
      "2018-12-26 16:05:39: Loss after num_examples_seen=900 epoch=9: 5.033748\n"
     ]
    }
   ],
   "source": [
    "#Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sdg(model, X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network with Theano and the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano as theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        # Theano: Created shared variables\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))      \n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        U, V, W = self.U, self.V, self.W\n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        def forward_prop_step(x_t, s_t_prev, U, V, W):\n",
    "            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n",
    "            o_t = T.nnet.softmax(V.dot(s_t))\n",
    "            return [o_t[0], s_t]\n",
    "        [o,s], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x,\n",
    "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
    "            non_sequences=[U, V, W],\n",
    "            truncate_gradient=self.bptt_truncate,\n",
    "            strict=True)\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Gradients\n",
    "        dU = T.grad(o_error, U)\n",
    "        dV = T.grad(o_error, V)\n",
    "        dW = T.grad(o_error, W)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.forward_propagation = theano.function([x], o)\n",
    "        self.predict = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], o_error)\n",
    "        self.bptt = theano.function([x, y], [dU, dV, dW])\n",
    "        \n",
    "        # SGD\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        self.sgd_step = theano.function([x,y,learning_rate], [], \n",
    "                      updates=[(self.U, self.U - learning_rate * dU),\n",
    "                              (self.V, self.V - learning_rate * dV),\n",
    "                              (self.W, self.W - learning_rate * dW)])\n",
    "    \n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_total_loss(X,Y)/float(num_words)   \n",
    "\n",
    "\n",
    "def gradient_check_theano(model, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Overwrite the bptt attribute. We need to backpropagate all the way to get the correct gradient\n",
    "    model.bptt_truncate = 1000\n",
    "    # Calculate the gradients using backprop\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to chec.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter_T = operator.attrgetter(pname)(model)\n",
    "        parameter = parameter_T.get_value()\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            parameter_T.set_value(parameter)\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            parameter_T.set_value(parameter)\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            parameter[ix] = original_value\n",
    "            parameter_T.set_value(parameter)\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 50.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 50.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 5\n",
    "model = RNNTheano(grad_check_vocab_size, 10)\n",
    "gradient_check_theano(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 17.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "model = RNNTheano(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-26 16:18:58: Loss after num_examples_seen=0 epoch=0: 7.313176\n",
      "2018-12-26 16:20:33: Loss after num_examples_seen=22230 epoch=10: 5.964751\n",
      "2018-12-26 16:22:08: Loss after num_examples_seen=44460 epoch=20: 5.781374\n",
      "2018-12-26 16:23:42: Loss after num_examples_seen=66690 epoch=30: 5.751034\n",
      "2018-12-26 16:25:16: Loss after num_examples_seen=88920 epoch=40: 5.843756\n",
      "Setting learning rate to 0.002500\n",
      "2018-12-26 16:26:50: Loss after num_examples_seen=111150 epoch=50: 5.720955\n",
      "2018-12-26 16:28:25: Loss after num_examples_seen=133380 epoch=60: 5.755570\n",
      "Setting learning rate to 0.001250\n",
      "2018-12-26 16:30:00: Loss after num_examples_seen=155610 epoch=70: 5.741841\n",
      "2018-12-26 16:31:35: Loss after num_examples_seen=177840 epoch=80: 5.756512\n",
      "Setting learning rate to 0.000625\n",
      "2018-12-26 16:33:10: Loss after num_examples_seen=200070 epoch=90: 5.773105\n",
      "Setting learning rate to 0.000313\n"
     ]
    }
   ],
   "source": [
    "model = RNNTheano(vocabulary_size, hidden_dim=50)\n",
    "losses = train_with_sdg(model, X_train, y_train, nepoch=100, evaluate_loss_after=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 15 minutes, but reached loss of 5.7 after 50 epochs and 8 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model parameters to data/trained_model_theano.npz.\n"
     ]
    }
   ],
   "source": [
    "save_model_parameters_theano('data/trained_model_theano.npz', model)\n",
    "#load_model_parameters_theano('data/trained_model_theano.npz', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model):\n",
    "    #Begin with the start token\n",
    "    new_sentence = [word2idx[sentence_start_token]]\n",
    "    #Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word2idx[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word2idx[unknown_token]\n",
    "        #We don't want to sample unknown words\n",
    "        while sampled_word == word2idx[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [idx2word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asked dorothy saved the tin woodman .\n",
      "good-bye the scarecrow woodman the north . ''\n",
      "asked the lion at that woodman watched .\n",
      "'' said the little happen wife .\n",
      "asked the tin woodman belonged him close she not both .\n",
      "asked it crops gorgeous it to head his frighten .\n",
      "asked dorothy 's dare quite weep and a little wizard .\n",
      "`` show to you groan ? ''\n",
      "'' cried same gratefully how grieved surely her place . ''\n",
      "cried dorothy been their wall munchkin wishes him and many country .\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 10\n",
    "sentence_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    #We want long sentences\n",
    "    while len(sent) < sentence_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:  \n",
    "* Learned to end sentences with punctuation  \n",
    "* Learned some adjectives that appear together often, eg tin woodman, little wizard  \n",
    "\n",
    "Cons:  \n",
    "* Quotation marks do not always end and do not make contextual sense\n",
    "* Did not learn commas *Note* they do show up but are infrequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asked the scarecrow tin today water once were farther .\n",
      "dorothy everything , dear him with a be witch .\n",
      "'' replied timidly fastened quiet dear seeing step reason picked guardian horses lady dried higher easy words wept straight experience loved sigh reach woman willingly modern child guardian crossed meet beyond killing greatly led princess now growing yes arose suppose now flock doorway gates offered animal returned certainly pulled among if fresh disappeared rubies touched dropped follow welcome discovered ashamed man joyfully bark battle bit fairy gravely will wink simply word maiden yet angrily say forced spectacles wipe forward brook understand mind n't simply allow willing handsome happened answer exclaimed without finally fly indeed thanking winkies munchkin scampered can marched mark bears mind respect yes fight yes blocks pick safely scampered ruler clean manner hear awakened awake ropes exclaimed amuse chattering strangers belonged quick might uncle uncomfortable perhaps shown witch , touched look indeed anxiously ruby cried give instead stretched lay boy heartily rooms led oh how tigers rested word pulled reason smooth north dare turned wisely pile stuffed prevent wizard will night love since third dropped munchkin courtyard creature paw 'll em keep fill spent spread gratefully blue chase yet sticking bed caught forgot sitting speak 'll child earth remained dear ca bird strangers room walk without others , gracious basket west spread badly run become bridges strangers move strangest promise heard struck crowd dear ! ''\n",
      "cried dorothy , simply to long left clumsy place .\n",
      "answered dorothy journey behind with take with it high hot contented of the emerald city .\n",
      "asked dorothy almost on with bark way him all . ''\n",
      "good-bye deal to stick come wildcat unable beautiful it . ''\n",
      "asked the lion his dark and her easily . ''\n",
      "asked dorothy declared sparkling like a top as wanted .\n",
      "asked see tin woodman supper green journey and seen with from green .\n",
      "asked dorothy to twisted any until him dorothy 's many stand bent .\n",
      "'' said cowardly stork understand some that of master told forgot hour she the home . ''\n",
      "cried dorothy well , in aunt bade her low .\n",
      "asked dorothy , tin beasts with buttercups spectacles . ''\n",
      "good-bye fellow greatly man his yellow ride and please .\n",
      "oz watched to you bodies a brains be cheeks .\n",
      "hanging tin woodman say closed threatened ladder square answer .\n",
      "asked dorothy would glinda do know open afraid of surprise .\n",
      "good-bye oh saving year in him whistle nodded black could the hurry .\n",
      "asked all tin woodman to the roadside fight wife .\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "sentence_min_length = 10\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    #We want long sentences\n",
    "    while len(sent) < sentence_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one enormous sentence!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
