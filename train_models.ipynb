{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from shksp_complete_works.txt\n",
      "Parsed 98720 sentences\n",
      "Found 32433 unique word tokens\n",
      "Using vocabulary size 13000\n",
      "The least frequent word in our vocabulary is \"maws\" appearing 2 times\n",
      "Example sentence: \"SENTENCE_START [to sicinius] you shall stay too. SENTENCE_END\"\n",
      "Example sentence after pre-processing: \"[u'SENTENCE_START', u'[', u'to', u'sicinius', u']', u'you', u'shall', u'stay', u'too', u'.', u'SENTENCE_END']\"\n",
      "X:\n",
      "SENTENCE_START UNKNOWN_TOKEN his heart , ' those are the very words .\n",
      "[0, 12999, 24, 128, 2, 52, 207, 49, 4, 152, 275, 3]\n",
      "y:\n",
      "UNKNOWN_TOKEN his heart , ' those are the very words . SENTENCE_END\n",
      "[12999, 24, 128, 2, 52, 207, 49, 4, 152, 275, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "corpus, tokenized_sentences, sentences, word_freq, vocab, idx2word, word2idx, X_train, y_train = preprocessing_pipeline('shksp_complete_works.txt', vocab_size=13000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gru_theano.py:64: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n",
      "  o_t = T.nnet.softmax(V.dot(s_t2) + c)[0]\n",
      "gru_theano.py:109: UserWarning: The Param class is deprecated. Replace Param(default=N) by theano.In(value=N)\n",
      "  [x, y, learning_rate, theano.Param(decay, default=0.9)],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD step time: ~8408.481121 millisenconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-334f0b152d1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'SGD step time: ~%f millisenconds'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_with_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/eric/progProj/rnn/utils.pyc\u001b[0m in \u001b[0;36mtrain_with_sgd\u001b[0;34m(model, X_train, y_train, learning_rate, nepoch, decay, callback_every, callback)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m# One SGD step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mnum_examples_seen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# Optionally do callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eric/envs/py2/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eric/envs/py2/local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eric/envs/py2/local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 20\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "model = GRUTheano(len(idx2word), HIDDEN_DIM)\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[0], y_train[0], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print 'SGD step time: ~%f millisenconds' % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train, y_train, LEARNING_RATE, NEPOCH, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_parameters_theano_gru(model, 'data/shksp_trained_model.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dickens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from dickens_bleak_house.txt\n",
      "Loaded text from dickens_christmas_carol.txt\n",
      "Loaded text from dickens_copperfield.txt\n",
      "Loaded text from dickens_great_exp.txt\n",
      "Loaded text from dickens_hard_times.txt\n",
      "Loaded text from dickens_little_dorrit.txt\n",
      "Loaded text from dickens_nickleby.txt\n",
      "Loaded text from dickens_oliver.txt\n",
      "Loaded text from dickens_pickwick.txt\n",
      "Loaded text from dickens_two_cities.txt\n",
      "Parsed 100179 sentences\n",
      "Found 41152 unique word tokens\n",
      "Using vocabulary size 21000\n",
      "The least frequent word in our vocabulary is \"unimaginable\" appearing 2 times\n",
      "Example sentence: \"SENTENCE_START when we had retired for the night, and ada and i had had our usual talk in our pretty room, i went out at my door again and sought my guardian among his books. SENTENCE_END\"\n",
      "Example sentence after pre-processing: \"[u'SENTENCE_START', u'when', u'we', u'had', u'retired', u'for', u'the', u'night', u',', u'and', u'ada', u'and', u'i', u'had', u'had', u'our', u'usual', u'talk', u'in', u'our', u'pretty', u'room', u',', u'i', u'went', u'out', u'at', u'my', u'door', u'again', u'and', u'sought', u'my', u'guardian', u'among', u'his', u'books', u'.', u'SENTENCE_END']\"\n",
      "X:\n",
      "SENTENCE_START it is no matter .\n",
      "[2, 14, 38, 50, 322, 4]\n",
      "y:\n",
      "it is no matter . SENTENCE_END\n",
      "[14, 38, 50, 322, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "corpus, tokenized_sentences, sentences, word_freq, vocab, idx2word, word2idx, X_train, y_train = preprocessing_pipeline(['dickens_bleak_house.txt', 'dickens_christmas_carol.txt', 'dickens_copperfield.txt', 'dickens_great_exp.txt', 'dickens_hard_times.txt', 'dickens_little_dorrit.txt', 'dickens_nickleby.txt', 'dickens_oliver.txt', 'dickens_pickwick.txt', 'dickens_two_cities.txt'], vocab_size=21000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 20\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "model = GRUTheano(len(idx2word), HIDDEN_DIM)\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[0], y_train[0], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print 'SGD step time: ~%f millisenconds' % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train, y_train, LEARNING_RATE, NEPOCH, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_parameters_theano_gru(model, 'data/dickens_trained_model.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from plato_apology_crito_phaedo.txt\n",
      "Loaded text from plato_euthyphro.txt\n",
      "Loaded text from plato_gorgias.txt\n",
      "Loaded text from plato_laws.txt\n",
      "Loaded text from plato_meno.txt\n",
      "Loaded text from plato_phaedrus.txt\n",
      "Loaded text from plato_protagoras.txt\n",
      "Loaded text from plato_republic.txt\n",
      "Loaded text from plato_symposium.txt\n",
      "Loaded text from plato_theaetetus.txt\n",
      "Loaded text from plato_timaeus.txt\n",
      "Parsed 20524 sentences\n",
      "Found 16895 unique word tokens\n",
      "Using vocabulary size 9000\n",
      "The least frequent word in our vocabulary is \"vestibule\" appearing 2 times\n",
      "Example sentence: \"SENTENCE_START at first the effect is unconscious; but when reason arrives, then he who has been thus trained welcomes her as the friend whom he always knew. SENTENCE_END\"\n",
      "Example sentence after pre-processing: \"[u'SENTENCE_START', u'at', u'first', u'the', u'effect', u'is', u'unconscious', u';', u'but', u'when', u'reason', u'arrives', u',', u'then', u'he', u'who', u'has', u'been', u'thus', u'trained', u'welcomes', u'her', u'as', u'the', u'friend', u'whom', u'he', u'always', u'knew', u'.', u'SENTENCE_END']\"\n",
      "X:\n",
      "SENTENCE_START certainly .\n",
      "[4, 201, 6]\n",
      "y:\n",
      "certainly . SENTENCE_END\n",
      "[201, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "corpus, tokenized_sentences, sentences, word_freq, vocab, idx2word, word2idx, X_train, y_train = preprocessing_pipeline(['plato_apology_crito_phaedo.txt', 'plato_euthyphro.txt', 'plato_gorgias.txt', 'plato_laws.txt', 'plato_meno.txt', 'plato_phaedrus.txt', 'plato_protagoras.txt', 'plato_republic.txt', 'plato_symposium.txt', 'plato_theaetetus.txt', 'plato_timaeus.txt'], vocab_size=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 20\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "model = GRUTheano(len(idx2word), HIDDEN_DIM)\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[0], y_train[0], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print 'SGD step time: ~%f millisenconds' % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train, y_train, LEARNING_RATE, NEPOCH, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_parameters_theano_gru(model, 'data/plato_trained_model.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shakespeare, Dickens and Plato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from dickens_bleak_house.txt\n",
      "Loaded text from dickens_christmas_carol.txt\n",
      "Loaded text from dickens_copperfield.txt\n",
      "Loaded text from dickens_great_exp.txt\n",
      "Loaded text from dickens_hard_times.txt\n",
      "Loaded text from dickens_little_dorrit.txt\n",
      "Loaded text from dickens_nickleby.txt\n",
      "Loaded text from dickens_oliver.txt\n",
      "Loaded text from dickens_pickwick.txt\n",
      "Loaded text from dickens_two_cities.txt\n",
      "Loaded text from plato_apology_crito_phaedo.txt\n",
      "Loaded text from plato_euthyphro.txt\n",
      "Loaded text from plato_gorgias.txt\n",
      "Loaded text from plato_laws.txt\n",
      "Loaded text from plato_meno.txt\n",
      "Loaded text from plato_phaedrus.txt\n",
      "Loaded text from plato_protagoras.txt\n",
      "Loaded text from plato_republic.txt\n",
      "Loaded text from plato_symposium.txt\n",
      "Loaded text from plato_theaetetus.txt\n",
      "Loaded text from plato_timaeus.txt\n",
      "Loaded text from shksp_complete_works.txt\n",
      "Parsed 219422 sentences\n",
      "Found 63216 unique word tokens\n",
      "Using vocabulary size 20000\n",
      "The least frequent word in our vocabulary is \"pliant\" appearing 5 times\n",
      "Example sentence: \"SENTENCE_START ‘oh! SENTENCE_END\"\n",
      "Example sentence after pre-processing: \"[u'SENTENCE_START', u'\\u2018', u'oh', u'!', u'SENTENCE_END']\"\n",
      "X:\n",
      "SENTENCE_START ‘ they may be incapable of it .\n",
      "[1, 18, 51, 104, 27, 3795, 6, 16, 3]\n",
      "y:\n",
      "‘ they may be incapable of it . SENTENCE_END\n",
      "[18, 51, 104, 27, 3795, 6, 16, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "corpus, tokenized_sentences, sentences, word_freq, vocab, idx2word, word2idx, X_train, y_train = preprocessing_pipeline(['dickens_bleak_house.txt', 'dickens_christmas_carol.txt', 'dickens_copperfield.txt', 'dickens_great_exp.txt', 'dickens_hard_times.txt', 'dickens_little_dorrit.txt', 'dickens_nickleby.txt', 'dickens_oliver.txt', 'dickens_pickwick.txt', 'dickens_two_cities.txt', 'plato_apology_crito_phaedo.txt', 'plato_euthyphro.txt', 'plato_gorgias.txt', 'plato_laws.txt', 'plato_meno.txt', 'plato_phaedrus.txt', 'plato_protagoras.txt', 'plato_republic.txt', 'plato_symposium.txt', 'plato_theaetetus.txt', 'plato_timaeus.txt', 'shksp_complete_works.txt'], vocab_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 20\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "model = GRUTheano(len(idx2word), HIDDEN_DIM)\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[0], y_train[0], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print 'SGD step time: ~%f millisenconds' % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train, y_train, LEARNING_RATE, NEPOCH, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_parameters_theano_gru(model, 'data/dickens_plato_shksp_trained_model.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the works of three authors separately and together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
