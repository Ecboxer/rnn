{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 1500\n",
    "UNKOWN_TOKEN = 'UNKNOWN_TOKEN'\n",
    "SENTENCE_START_TOKEN = 'SENTENCE_START'\n",
    "SENTENCE_END_TOKEN = 'SENTENCE_END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "f = open('wizOfOz.txt', 'r')\n",
    "#Read corpus into text as list of lines\n",
    "ast_count = 0\n",
    "for line in f.readlines():\n",
    "    if ast_count == 0: #Gutenberg text\n",
    "        if line[0] == '*':\n",
    "            ast_count += 1\n",
    "    elif ast_count == 1: #Book text\n",
    "        if line[0] == '*': #Gutenberg text\n",
    "            ast_count += 1\n",
    "        else:\n",
    "            text.append(line)\n",
    "f.close()\n",
    "\n",
    "#Form corpus by joining list of lines\n",
    "corpus = ''.join(text)\n",
    "\n",
    "#Remove line breaks and returns\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "corpus = corpus.replace('\\r', '')\n",
    "corpus = corpus.replace('\\\\', '')\n",
    "\n",
    "#Remove multiple whitespace\n",
    "corp = ' '.join(corpus.split())\n",
    "\n",
    "#Start from the introduction, the second occurrence of the word Introduction\n",
    "#iter = re.finditer(r'Introduction', corpus)\n",
    "#intro_indices = [m.start(0) for m in iter]\n",
    "#corp = corpus[intro_indices[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split corpus in to sentences\n",
    "sentences = nltk.sent_tokenize(corp.decode('utf-8').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append SENTENCE_START and SENTENCE_END\n",
    "sentences = ['%s %s %s' % (SENTENCE_START_TOKEN, x, SENTENCE_END_TOKEN) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2223 sentences\n"
     ]
    }
   ],
   "source": [
    "print 'Parsed %d sentences' % (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2959 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "#Count word frequency\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print 'Found %d unique word tokens' % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 1500\n",
      "The least frequent word in our vocabulary is \"tremble\" appearing 2 times\n"
     ]
    }
   ],
   "source": [
    "#Get the most common words and build idx2word and word2idx vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "idx2word = [x[0] for x in vocab]\n",
    "idx2word.append(UNKNOWN_TOKEN)\n",
    "word2idx = dict([(w,i) for i,w in enumerate(idx2word)])\n",
    "print 'Using vocabulary size %d' % vocabulary_size\n",
    "print 'The least frequent word in our vocabulary is \"%s\" appearing %d times' % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all words not in our vocabulary with the unknown token\n",
    "for i,sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word2idx else UNKNOWN_TOKEN for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: \"SENTENCE_START if you, who are great and terrible, cannot kill her yourself, how do you expect me to do it?\" SENTENCE_END\"\n",
      "Example sentence after pre-processing \"[u'SENTENCE_START', u'if', u'you', u',', u'who', u'are', u'great', u'and', u'terrible', u',', u'can', u'not', u'kill', u'her', u'yourself', u',', u'how', u'do', u'you', u'expect', u'me', u'to', u'do', u'it', u'?', u\"''\", u'SENTENCE_END']\"\n"
     ]
    }
   ],
   "source": [
    "temp = np.random.randint(0, len(sentences))\n",
    "print 'Example sentence: \"%s\"\\nExample sentence after pre-processing \"%s\"' % (sentences[temp], tokenized_sentences[temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training data\n",
    "X_train = np.asarray([[word2idx[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word2idx[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START aunt em dropped her work and came to the door .\n",
      "[3, 219, 250, 812, 17, 287, 5, 70, 6, 0, 242, 4]\n",
      "y:\n",
      "aunt em dropped her work and came to the door . SENTENCE_END\n",
      "[219, 250, 812, 17, 287, 5, 70, 6, 0, 242, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "#Training data example\n",
    "temp = np.random.randint(0, len(sentences))\n",
    "x_example, y_example = X_train[temp], y_train[temp]\n",
    "print 'x:\\n%s\\n%s' % (' '.join([idx2word[x] for x in x_example]), x_example)\n",
    "print 'y:\\n%s\\n%s' % (' '.join([idx2word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "from theano.gradient import grad_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUTheano(vocabulary_size, hidden_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 5\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[0], y_train[0], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print 'SGD step time: ~%f millisenconds' % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train, y_train, LEARNING_RATE, NEPOCH, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model model from data/trained_model_theano_gru.npz with hidden_dim=128 word_dim=1500\n"
     ]
    }
   ],
   "source": [
    "#save_model_parameters_theano_gru(model, 'data/trained_model_theano_gru.npz')\n",
    "model = load_model_parameters_theano_gru('data/trained_model_theano_gru.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "too asleep , day the tall a to bed to there to be of throne , and things dorothy , other looked know finally .\n",
      "`` great a taken wondering , i mind all , be how baum little wall with any while you tin then . ''\n",
      "`` said , replied i it was , and very started to cap is . ''\n",
      "`` do is that balloon ! ''\n",
      "`` i by that had should .\n",
      "`` that oz is a will will pleased of my come city , i they would a are joints she as have as been everything . ''\n",
      "the once was never as she after the `` woman n't be you branches ? ''\n",
      "on the lion stopped he part that bad she was only country to well as oz could fast wonderful one point the dorothy much would ; to his did they went to made her her when the forest .\n",
      "`` i and 's continued , i feet to be mice , little of lion . ''\n",
      "`` he saved us for the when hill the body no it .\n",
      "`` i was help down of with it , my will him . ''\n",
      "back the i can gloomy , do .\n",
      "to his one `` my which ? ''\n",
      "so the middle was him in the farther hil-lo to had been back , and courage to you a .\n",
      "`` ordered you are needles oil-can you ? ''\n",
      "were beautiful long sights , glinda for in on the '' wanted back a big wanted .\n",
      "`` , their by , i will give the than big into his went , and i are not with visit listened to well beside it that seat of no .\n",
      "have be the balloon , scarecrow when '' they his `` was , the scarecrow and said the command for the it party .\n",
      "and asked once the not day , so she live air at the scarecrow and her having on by any we must oh of woman ; . arms\n",
      "`` these was is , i am put the down , and fellow a promises last you perhaps what do you legs ? ''\n",
      "so then the make her when here the was you once , for i promises she and matter\n",
      "dorothy was willing to must him there that so on and he was '' is together , `` for you to them dressed with they never we shall as on the still all first greatly be , for i pretty there , is n't queen , and i into do i can a some my mice gates but the while you .\n",
      "dorothy said saw there through the dangerous , you now of a very is munchkins in the ! ''\n",
      "i did not give you down pleased to , '' time the shall one if as 's us , if he was about the great they off in the wall and the road of the fastened .\n",
      "us yes yellow herself on at why , thanking animals and sky in go .\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(model, 25, idx2word, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences are not coherent. I will try to train for longer on AWS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
