{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 1500\n",
    "UNKOWN_TOKEN = 'UNKNOWN_TOKEN'\n",
    "SENTENCE_START_TOKEN = 'SENTENCE_START'\n",
    "SENTENCE_END_TOKEN = 'SENTENCE_END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "f = open('wizOfOz.txt', 'r')\n",
    "#Read corpus into text as list of lines\n",
    "ast_count = 0\n",
    "for line in f.readlines():\n",
    "    if ast_count == 0: #Gutenberg text\n",
    "        if line[0] == '*':\n",
    "            ast_count += 1\n",
    "    elif ast_count == 1: #Book text\n",
    "        if line[0] == '*': #Gutenberg text\n",
    "            ast_count += 1\n",
    "        else:\n",
    "            text.append(line)\n",
    "f.close()\n",
    "\n",
    "#Form corpus by joining list of lines\n",
    "corpus = ''.join(text)\n",
    "\n",
    "#Remove line breaks and returns\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "corpus = corpus.replace('\\r', '')\n",
    "corpus = corpus.replace('\\\\', '')\n",
    "\n",
    "#Remove multiple whitespace\n",
    "corp = ' '.join(corpus.split())\n",
    "\n",
    "#Start from the introduction, the second occurrence of the word Introduction\n",
    "#iter = re.finditer(r'Introduction', corpus)\n",
    "#intro_indices = [m.start(0) for m in iter]\n",
    "#corp = corpus[intro_indices[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split corpus in to sentences\n",
    "sentences = nltk.sent_tokenize(corp.decode('utf-8').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append SENTENCE_START and SENTENCE_END\n",
    "sentences = ['%s %s %s' % (SENTENCE_START_TOKEN, x, SENTENCE_END_TOKEN) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2223 sentences\n"
     ]
    }
   ],
   "source": [
    "print 'Parsed %d sentences' % (len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2959 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "#Count word frequency\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print 'Found %d unique word tokens' % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 1500\n",
      "The least frequent word in our vocabulary is \"tremble\" appearing 2 times\n"
     ]
    }
   ],
   "source": [
    "#Get the most common words and build idx2word and word2idx vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "idx2word = [x[0] for x in vocab]\n",
    "idx2word.append(UNKNOWN_TOKEN)\n",
    "word2idx = dict([(w,i) for i,w in enumerate(idx2word)])\n",
    "print 'Using vocabulary size %d' % vocabulary_size\n",
    "print 'The least frequent word in our vocabulary is \"%s\" appearing %d times' % (vocab[-1][0], vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all words not in our vocabulary with the unknown token\n",
    "for i,sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word2idx else UNKNOWN_TOKEN for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: \"SENTENCE_START \"i am now worse off than when i first met dorothy,\" he thought. SENTENCE_END\"\n",
      "Example sentence after pre-processing \"[u'SENTENCE_START', u'``', u'i', u'am', u'now', u'worse', u'off', u'than', u'when', u'i', u'first', u'met', u'dorothy', u',', u\"''\", u'he', u'thought', u'.', u'SENTENCE_END']\"\n"
     ]
    }
   ],
   "source": [
    "temp = np.random.randint(0, len(sentences))\n",
    "print 'Example sentence: \"%s\"\\nExample sentence after pre-processing \"%s\"' % (sentences[temp], tokenized_sentences[temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training data\n",
    "X_train = np.asarray([[word2idx[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word2idx[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START `` UNKNOWN_TOKEN , my dear , '' he said .\n",
      "[3, 7, 1499, 1, 40, 398, 1, 8, 15, 22, 4]\n",
      "y:\n",
      "`` UNKNOWN_TOKEN , my dear , '' he said . SENTENCE_END\n",
      "[7, 1499, 1, 40, 398, 1, 8, 15, 22, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "#Training data example\n",
    "temp = np.random.randint(0, len(sentences))\n",
    "x_example, y_example = X_train[temp], y_train[temp]\n",
    "print 'x:\\n%s\\n%s' % (' '.join([idx2word[x] for x in x_example]), x_example)\n",
    "print 'y:\\n%s\\n%s' % (' '.join([idx2word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano as theano\n",
    "import theano.tensor as T\n",
    "from theano.gradient import grad_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUTheano(vocabulary_size, hidden_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NEPOCH = 5\n",
    "\n",
    "t1 = time.time()\n",
    "model.sgd_step(X_train[0], y_train[0], LEARNING_RATE)\n",
    "t2 = time.time()\n",
    "print 'SGD step time: ~%f millisenconds' % ((t2 - t1) * 1000.)\n",
    "\n",
    "train_with_sgd(model, X_train, y_train, LEARNING_RATE, NEPOCH, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model model from data/trained_model_theano_gru.npz with hidden_dim=128 word_dim=1500\n"
     ]
    }
   ],
   "source": [
    "#save_model_parameters_theano_gru(model, 'data/trained_model_theano_gru.npz')\n",
    "model = load_model_parameters_theano_gru('data/trained_model_theano_gru.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "were whatever was old dreadful , and forget man over miles a coming can flew of is forward .\n",
      "silk are a by tree ; he think but he she this is a world to to of fearing the fun of whiskers oz , for the scarecrow beast . ''\n",
      "`` what of am heels , i us sorry\n",
      "the luck so i will be climbed looked woodman , and i will road in the do one of heavy you help themselves for i , i can me my back to you .\n",
      "`` if you must oh to me . ''\n",
      "again he oats a other scarecrow to toto , how what\n",
      "said so the shoe was .\n",
      "who was , being answered , `` country the cellar and she looked a part of oz . ''\n",
      "`` be fell to .\n",
      "so at that the so people then it scarecrow , emerald no brains .\n",
      "`` i who his up to me to why again you empty contented , '' said the where .\n",
      "but he told you to must seen him green .\n",
      "`` who spread my again . ''\n",
      "`` we can not word but you and me there . ''\n",
      "`` she was make her , and streets is perhaps things , to we while i came .\n",
      "she asked her i listened , and the asked and the tin woodman was with quite ; and he would when he was a at fearing on it .\n",
      "lying she hanging thankful the because of her away , and over her i used to answered far : surrounded her in the careful , we were will to then out , also of sheep to aunt as but surprise . him\n",
      "time , she he day woodman into but the well this tin witch to the have they into the dorothy small empty to hole , and of oz .\n",
      "table the at but the scarecrow , on screen , he ; i heard laugh from the swiftly of the eyes we now upon their .\n",
      "`` daylight is no good that shoe as the scarecrow , '' powerful give the as screamed .\n",
      "`` i those boq and your gingham ? ''\n",
      "the kansas nuts to that the green this down had\n",
      "the scarecrow began to him , and at as the prairies this no over the scarecrow them back witch , and as they is '' dorothy first in a first and saw green and quite hill to great you her got after of them .\n",
      "as the we as he land .\n",
      ", the my when they dorothy a was have lovely up scarecrow , `` hurt , you busy , and i you had their `` head am to wondering and ask down will be this . ''\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(model, 25, idx2word, word2idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
